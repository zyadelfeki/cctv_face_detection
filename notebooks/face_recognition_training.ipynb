{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29568dae",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected! Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12653ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages - using Colab-compatible versions\n",
    "# We avoid facenet-pytorch due to numpy version conflicts\n",
    "\n",
    "!pip install -q kaggle wandb\n",
    "!pip install -q albumentations\n",
    "\n",
    "# Install MTCNN separately (face detection)\n",
    "!pip install -q mtcnn\n",
    "\n",
    "print(\"âœ… Packages installed! Restarting runtime...\")\n",
    "\n",
    "# Auto-restart to ensure clean state\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "# Face Detection (MTCNN uses TensorFlow backend in this version)\n",
    "from mtcnn import MTCNN\n",
    "\n",
    "# Image Processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ… All imports successful!\")\n",
    "print(f\"ðŸ“ Using device: {DEVICE}\")\n",
    "print(f\"ðŸ“¦ NumPy version: {np.__version__}\")\n",
    "print(f\"ðŸ“¦ PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59763c",
   "metadata": {},
   "source": [
    "## 2. Kaggle Setup & Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e99c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kaggle credentials from the kaggle.json file in the notebooks folder\n",
    "# This file is bundled with the project - no need to manually enter credentials\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Kaggle credentials (from notebooks/kaggle.json)\n",
    "KAGGLE_USERNAME = \"zyadelfeki1\"\n",
    "KAGGLE_KEY = \"0ca3cf05892a2d79ecc9fe7f6ae0d05e\"\n",
    "\n",
    "# Create Kaggle config directory and file\n",
    "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "kaggle_config = {\"username\": KAGGLE_USERNAME, \"key\": KAGGLE_KEY}\n",
    "\n",
    "with open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as f:\n",
    "    json.dump(kaggle_config, f)\n",
    "\n",
    "os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
    "\n",
    "print(\"âœ… Kaggle credentials configured automatically!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset directly from Kaggle to Colab (no local download needed)\n",
    "# The dataset stays on Colab's cloud storage, not your local machine\n",
    "\n",
    "# Option 1: Labeled Faces in the Wild (LFW) - Classic benchmark\n",
    "# !kaggle datasets download -d jessicali9530/lfw-dataset -p ./data --unzip\n",
    "\n",
    "# Option 2: Celebrity Face Dataset - Good for recognition training\n",
    "# !kaggle datasets download -d vishesh1412/celebrity-face-image-dataset -p ./data --unzip\n",
    "\n",
    "# Option 3: FFHQ Face Dataset (default) - High quality faces\n",
    "!kaggle datasets download -d greatgamedota/ffhq-face-data-set -p ./data --unzip\n",
    "\n",
    "# The -p flag downloads directly to ./data\n",
    "# The --unzip flag extracts automatically (no separate unzip step)\n",
    "# Data stays in Colab's ephemeral storage, never downloaded to your local machine\n",
    "\n",
    "print(\"\\nðŸ“ Dataset downloaded to Colab cloud storage!\")\n",
    "print(\"(Data is NOT on your local machine - it's in Colab's temporary storage)\")\n",
    "!ls -la ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Use a custom dataset structure\n",
    "# Expected structure:\n",
    "# data/\n",
    "#   person_1/\n",
    "#     img1.jpg\n",
    "#     img2.jpg\n",
    "#   person_2/\n",
    "#     img1.jpg\n",
    "#     ...\n",
    "\n",
    "DATA_DIR = Path(\"./data\")\n",
    "\n",
    "def explore_dataset(data_dir: Path) -> List[Dict]:\n",
    "    \"\"\"Explore dataset structure and create a manifest.\"\"\"\n",
    "    from collections import Counter\n",
    "    records = []\n",
    "    \n",
    "    for person_dir in data_dir.iterdir():\n",
    "        if person_dir.is_dir():\n",
    "            person_name = person_dir.name\n",
    "            images = list(person_dir.glob(\"*.jpg\")) + list(person_dir.glob(\"*.png\")) + list(person_dir.glob(\"*.jpeg\"))\n",
    "            for img_path in images:\n",
    "                records.append({\n",
    "                    \"person\": person_name,\n",
    "                    \"image_path\": str(img_path),\n",
    "                    \"image_name\": img_path.name\n",
    "                })\n",
    "    \n",
    "    # Statistics\n",
    "    person_counts = Counter(r['person'] for r in records)\n",
    "    counts = list(person_counts.values())\n",
    "    \n",
    "    print(f\"ðŸ“Š Dataset Summary:\")\n",
    "    print(f\"   Total images: {len(records)}\")\n",
    "    print(f\"   Total persons: {len(person_counts)}\")\n",
    "    print(f\"   Images per person - min: {min(counts)}, max: {max(counts)}, avg: {sum(counts)/len(counts):.1f}\")\n",
    "    \n",
    "    return records\n",
    "\n",
    "# Explore the dataset\n",
    "df_dataset = explore_dataset(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a28787f",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing & Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MTCNN for face detection (TensorFlow-based)\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "# Face preprocessing config\n",
    "FACE_SIZE = 160\n",
    "FACE_MARGIN = 20\n",
    "\n",
    "print(\"âœ… MTCNN initialized for face detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_align_face(image_path: str, detector: MTCNN, face_size: int = 160, margin: int = 20) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Detect and align face from image using MTCNN.\n",
    "    Returns face image as numpy array (face_size x face_size x 3) or None if no face detected.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces\n",
    "        detections = detector.detect_faces(img_rgb)\n",
    "        \n",
    "        if not detections:\n",
    "            return None\n",
    "        \n",
    "        # Get the largest face\n",
    "        largest_face = max(detections, key=lambda x: x['box'][2] * x['box'][3])\n",
    "        \n",
    "        # Extract bounding box with margin\n",
    "        x, y, w, h = largest_face['box']\n",
    "        x = max(0, x - margin)\n",
    "        y = max(0, y - margin)\n",
    "        w = w + 2 * margin\n",
    "        h = h + 2 * margin\n",
    "        \n",
    "        # Crop and resize face\n",
    "        face = img_rgb[y:y+h, x:x+w]\n",
    "        if face.size == 0:\n",
    "            return None\n",
    "            \n",
    "        face = cv2.resize(face, (face_size, face_size))\n",
    "        \n",
    "        return face\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def preprocess_dataset(records: List[Dict], detector: MTCNN, output_dir: Path) -> List[Dict]:\n",
    "    \"\"\"Detect faces and save aligned face crops.\"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    valid_records = []\n",
    "    \n",
    "    for record in tqdm(records, desc=\"Processing faces\"):\n",
    "        face = detect_and_align_face(record['image_path'], detector)\n",
    "        \n",
    "        if face is not None:\n",
    "            # Save aligned face\n",
    "            person_dir = output_dir / record['person']\n",
    "            person_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            face_path = person_dir / record['image_name']\n",
    "            \n",
    "            # Save as RGB image\n",
    "            Image.fromarray(face).save(face_path)\n",
    "            \n",
    "            valid_records.append({\n",
    "                \"person\": record['person'],\n",
    "                \"face_path\": str(face_path)\n",
    "            })\n",
    "    \n",
    "    return valid_records\n",
    "\n",
    "# Process dataset\n",
    "PROCESSED_DIR = Path(\"./processed_faces\")\n",
    "face_records = preprocess_dataset(df_dataset, mtcnn, PROCESSED_DIR)\n",
    "\n",
    "# Use the list directly\n",
    "df_faces = face_records\n",
    "\n",
    "print(f\"\\nâœ… Processed {len(face_records)} faces from {len(set(r['person'] for r in face_records))} persons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93030324",
   "metadata": {},
   "source": [
    "## 4. Custom Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6bfa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    \"\"\"Custom dataset for face recognition training.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        records: List[Dict],\n",
    "        transform=None,\n",
    "        label_encoder=None\n",
    "    ):\n",
    "        self.records = records\n",
    "        self.transform = transform\n",
    "        \n",
    "        persons = [r['person'] for r in records]\n",
    "        \n",
    "        if label_encoder is None:\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            self.labels = self.label_encoder.fit_transform(persons)\n",
    "        else:\n",
    "            self.label_encoder = label_encoder\n",
    "            self.labels = self.label_encoder.transform(persons)\n",
    "        \n",
    "        self.num_classes = len(self.label_encoder.classes_)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        record = self.records[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img = cv2.imread(record['face_path'])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented['image']\n",
    "        else:\n",
    "            img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "\n",
    "# Data Augmentation\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.3),\n",
    "    A.CoarseDropout(max_holes=8, max_height=16, max_width=16, p=0.3),\n",
    "    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Split dataset\n",
    "persons = [r['person'] for r in df_faces]\n",
    "train_records, val_records = train_test_split(\n",
    "    df_faces,\n",
    "    test_size=0.2,\n",
    "    stratify=persons,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_records)}, Val: {len(val_records)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FaceDataset(train_records, transform=train_transform)\n",
    "val_dataset = FaceDataset(val_records, transform=val_transform, label_encoder=train_dataset.label_encoder)\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… DataLoaders ready!\")\n",
    "print(f\"   Number of classes: {train_dataset.num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c83842",
   "metadata": {},
   "source": [
    "## 5. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b33eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceRecognitionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Face recognition model using ResNet50 backbone with custom embedding head.\n",
    "    \n",
    "    Architecture:\n",
    "    - Backbone: ResNet50 (pretrained on ImageNet, fine-tuned for faces)\n",
    "    - Head: ArcFace-style classifier for better angular margin\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        embedding_size: int = 512,\n",
    "        dropout: float = 0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ResNet50 backbone (pretrained on ImageNet)\n",
    "        backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        \n",
    "        # Remove the final FC layer\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        backbone_out_features = 2048\n",
    "        \n",
    "        # Freeze early layers for transfer learning\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            # Unfreeze layer3 and layer4 only\n",
    "            if 'layer3' not in name and 'layer4' not in name:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Custom embedding head\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding_layer = nn.Linear(backbone_out_features, embedding_size)\n",
    "        self.bn = nn.BatchNorm1d(embedding_size)\n",
    "        \n",
    "        # Classifier (for training with ArcFace-style loss)\n",
    "        self.classifier = nn.Linear(embedding_size, num_classes)\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, x, return_embeddings: bool = False):\n",
    "        # Backbone features\n",
    "        features = self.backbone(x)\n",
    "        features = self.flatten(features)\n",
    "        \n",
    "        # Embedding\n",
    "        features = self.dropout(features)\n",
    "        embeddings = self.embedding_layer(features)\n",
    "        embeddings = self.bn(embeddings)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return embeddings\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(embeddings)\n",
    "        \n",
    "        return logits, embeddings\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        \"\"\"Get normalized face embedding for inference.\"\"\"\n",
    "        return self.forward(x, return_embeddings=True)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = FaceRecognitionModel(\n",
    "    num_classes=train_dataset.num_classes,\n",
    "    embedding_size=512,\n",
    "    dropout=0.3\n",
    ").to(DEVICE)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nðŸ“ Model Summary:\")\n",
    "print(f\"   Backbone: ResNet50 (ImageNet pretrained)\")\n",
    "print(f\"   Embedding size: 512\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a49bc7",
   "metadata": {},
   "source": [
    "## 6. Loss Functions & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d7d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    ArcFace loss for better face recognition.\n",
    "    Adds angular margin to the softmax loss for tighter clusters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scale: float = 30.0, margin: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, logits, labels):\n",
    "        # Get cosine values\n",
    "        cos_theta = logits\n",
    "        \n",
    "        # Add margin to target class\n",
    "        one_hot = F.one_hot(labels, num_classes=logits.size(1)).float()\n",
    "        \n",
    "        # Compute arccos, add margin, then cos again\n",
    "        theta = torch.acos(torch.clamp(cos_theta, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "        marginal_cos = torch.cos(theta + self.margin * one_hot)\n",
    "        \n",
    "        # Scale and compute loss\n",
    "        scaled_logits = self.scale * marginal_cos\n",
    "        loss = self.ce(scaled_logits, labels)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combined CrossEntropy + Triplet Loss for robust training.\"\"\"\n",
    "    \n",
    "    def __init__(self, margin: float = 0.3, ce_weight: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.triplet = nn.TripletMarginLoss(margin=margin)\n",
    "        self.ce_weight = ce_weight\n",
    "    \n",
    "    def forward(self, logits, embeddings, labels):\n",
    "        ce_loss = self.ce(logits, labels)\n",
    "        \n",
    "        # Simple triplet mining within batch\n",
    "        triplet_loss = self._batch_hard_triplet_loss(embeddings, labels)\n",
    "        \n",
    "        return self.ce_weight * ce_loss + (1 - self.ce_weight) * triplet_loss\n",
    "    \n",
    "    def _batch_hard_triplet_loss(self, embeddings, labels, margin=0.3):\n",
    "        \"\"\"Compute triplet loss with batch hard mining.\"\"\"\n",
    "        # Pairwise distances\n",
    "        dist_matrix = torch.cdist(embeddings, embeddings)\n",
    "        \n",
    "        # Create masks\n",
    "        labels = labels.unsqueeze(1)\n",
    "        same_label = labels == labels.T\n",
    "        diff_label = ~same_label\n",
    "        \n",
    "        # Hardest positive: max distance within same class\n",
    "        pos_dist = dist_matrix * same_label.float()\n",
    "        hardest_pos, _ = pos_dist.max(dim=1)\n",
    "        \n",
    "        # Hardest negative: min distance to different class\n",
    "        neg_dist = dist_matrix + 1e9 * same_label.float()  # Mask same class\n",
    "        hardest_neg, _ = neg_dist.min(dim=1)\n",
    "        \n",
    "        # Triplet loss\n",
    "        loss = F.relu(hardest_pos - hardest_neg + margin).mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "# Setup training\n",
    "criterion = CombinedLoss(margin=0.3, ce_weight=0.7)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=5,\n",
    "    T_mult=2\n",
    ")\n",
    "\n",
    "print(\"âœ… Loss and optimizer configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb949e9d",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, embeddings = model(images)\n",
    "        loss = criterion(logits, embeddings, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = logits.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'acc': f\"{100.*correct/total:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc=\"Validating\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits, embeddings = model(images)\n",
    "        loss = criterion(logits, embeddings, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, predicted = logits.max(1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_embeddings = torch.cat(all_embeddings).numpy()\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(loader),\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'embeddings': all_embeddings,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f3a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 30\n",
    "SAVE_DIR = Path(\"./checkpoints\")\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "best_val_acc = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(\"ðŸš€ Starting training...\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_results = validate(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Val Loss: {val_results['loss']:.4f} | Val Acc: {val_results['accuracy']*100:.2f}%\")\n",
    "    print(f\"Val F1: {val_results['f1']:.4f}\")\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_results['loss'])\n",
    "    history['val_acc'].append(val_results['accuracy'])\n",
    "    \n",
    "    # Save best model\n",
    "    if val_results['accuracy'] > best_val_acc:\n",
    "        best_val_acc = val_results['accuracy']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_accuracy': best_val_acc,\n",
    "            'num_classes': model.num_classes,\n",
    "            'label_encoder_classes': train_dataset.label_encoder.classes_.tolist()\n",
    "        }, SAVE_DIR / \"best_model.pth\")\n",
    "        print(f\"ðŸ’¾ Saved best model with accuracy: {best_val_acc*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nâœ… Training complete! Best validation accuracy: {best_val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f58d9c",
   "metadata": {},
   "source": [
    "## 8. Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot([a*100 for a in history['train_acc']], label='Train Acc')\n",
    "axes[1].plot([a*100 for a in history['val_acc']], label='Val Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training & Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAVE_DIR / \"training_history.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "val_results = validate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "cm = confusion_matrix(val_results['labels'], val_results['predictions'])\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=False, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig(SAVE_DIR / \"confusion_matrix.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d60effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE Visualization of Embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Subsample for visualization\n",
    "n_samples = min(1000, len(val_results['embeddings']))\n",
    "indices = np.random.choice(len(val_results['embeddings']), n_samples, replace=False)\n",
    "\n",
    "embeddings_subset = val_results['embeddings'][indices]\n",
    "labels_subset = val_results['labels'][indices]\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=SEED, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(embeddings_subset)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=labels_subset,\n",
    "    cmap='tab20',\n",
    "    alpha=0.7,\n",
    "    s=30\n",
    ")\n",
    "plt.title('t-SNE Visualization of Face Embeddings')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.colorbar(scatter, label='Person ID')\n",
    "plt.savefig(SAVE_DIR / \"tsne_embeddings.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de2264",
   "metadata": {},
   "source": [
    "## 9. Export Models for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed951e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(SAVE_DIR / \"best_model.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ… Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"   Validation accuracy: {checkpoint['val_accuracy']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa4b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to TorchScript for production\n",
    "class EmbeddingModel(nn.Module):\n",
    "    \"\"\"Wrapper for embedding-only inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, full_model):\n",
    "        super().__init__()\n",
    "        self.backbone = full_model.backbone\n",
    "        self.dropout = full_model.dropout\n",
    "        self.embedding_layer = full_model.embedding_layer\n",
    "        self.bn = full_model.bn\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = self.dropout(features)\n",
    "        embeddings = self.embedding_layer(features)\n",
    "        embeddings = self.bn(embeddings)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# Create embedding model\n",
    "embedding_model = EmbeddingModel(model).to(DEVICE).eval()\n",
    "\n",
    "# Trace the model\n",
    "example_input = torch.randn(1, 3, 160, 160).to(DEVICE)\n",
    "traced_model = torch.jit.trace(embedding_model, example_input)\n",
    "\n",
    "# Save\n",
    "traced_model.save(SAVE_DIR / \"face_embedding_model.pt\")\n",
    "print(\"âœ… Saved TorchScript model: face_embedding_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541a2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX for cross-platform deployment\n",
    "import torch.onnx\n",
    "\n",
    "torch.onnx.export(\n",
    "    embedding_model,\n",
    "    example_input,\n",
    "    SAVE_DIR / \"face_embedding_model.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=12,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['embedding'],\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},\n",
    "        'embedding': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"âœ… Saved ONNX model: face_embedding_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save label encoder and metadata\n",
    "import json\n",
    "\n",
    "metadata = {\n",
    "    \"model_version\": \"1.0.0\",\n",
    "    \"embedding_size\": model.embedding_size,\n",
    "    \"num_classes\": model.num_classes,\n",
    "    \"input_size\": [3, 160, 160],\n",
    "    \"normalization\": {\n",
    "        \"mean\": [0.5, 0.5, 0.5],\n",
    "        \"std\": [0.5, 0.5, 0.5]\n",
    "    },\n",
    "    \"classes\": checkpoint['label_encoder_classes'],\n",
    "    \"training\": {\n",
    "        \"best_epoch\": checkpoint['epoch'],\n",
    "        \"val_accuracy\": checkpoint['val_accuracy']\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(SAVE_DIR / \"model_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"âœ… Saved model metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be93d72",
   "metadata": {},
   "source": [
    "## 10. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4069ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_embedding(image_path: str, model, mtcnn, device, transform) -> Optional[np.ndarray]:\n",
    "    \"\"\"Get face embedding from image.\"\"\"\n",
    "    # Detect face\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    face = mtcnn(img)\n",
    "    \n",
    "    if face is None:\n",
    "        return None\n",
    "    \n",
    "    # Preprocess\n",
    "    face_np = (face.permute(1, 2, 0).cpu().numpy() * 128 + 127.5).astype(np.uint8)\n",
    "    augmented = transform(image=face_np)\n",
    "    face_tensor = augmented['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model.get_embedding(face_tensor)\n",
    "    \n",
    "    return embedding.cpu().numpy()[0]\n",
    "\n",
    "\n",
    "def compute_similarity(emb1: np.ndarray, emb2: np.ndarray) -> float:\n",
    "    \"\"\"Compute cosine similarity between two embeddings.\"\"\"\n",
    "    return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "\n",
    "\n",
    "# Example: Compare two faces\n",
    "# emb1 = get_face_embedding(\"path/to/face1.jpg\", model, mtcnn, DEVICE, val_transform)\n",
    "# emb2 = get_face_embedding(\"path/to/face2.jpg\", model, mtcnn, DEVICE, val_transform)\n",
    "# similarity = compute_similarity(emb1, emb2)\n",
    "# print(f\"Similarity: {similarity:.4f}\")\n",
    "# print(f\"Same person: {similarity > 0.6}\")\n",
    "\n",
    "print(\"ðŸ“¥ Inference functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca1c07",
   "metadata": {},
   "source": [
    "## 11. Download Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c0c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip of all outputs\n",
    "import shutil\n",
    "\n",
    "# Zip the checkpoints directory\n",
    "shutil.make_archive(\"trained_models\", 'zip', SAVE_DIR)\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download(\"trained_models.zip\")\n",
    "\n",
    "print(\"\\nðŸ“¥ Download complete! Contents:\")\n",
    "print(\"   - best_model.pth (PyTorch checkpoint)\")\n",
    "print(\"   - face_embedding_model.pt (TorchScript)\")\n",
    "print(\"   - face_embedding_model.onnx (ONNX)\")\n",
    "print(\"   - model_metadata.json\")\n",
    "print(\"   - training_history.png\")\n",
    "print(\"   - confusion_matrix.png\")\n",
    "print(\"   - tsne_embeddings.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d70bda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download the trained models** using the cell above\n",
    "2. **Copy to your CCTV system**: Place models in `models/` directory\n",
    "3. **Update config.yaml** to point to the new model\n",
    "4. **Build FAISS index** from criminal database embeddings\n",
    "\n",
    "### Integration with CCTV System:\n",
    "\n",
    "```python\n",
    "# In your CCTV system's recognition module:\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import torch\n",
    "\n",
    "# Load custom trained model\n",
    "model = torch.jit.load('models/face_embedding_model.pt')\n",
    "model.eval()\n",
    "\n",
    "# Get embedding for new face\n",
    "embedding = model(face_tensor)\n",
    "\n",
    "# Compare with criminal database using FAISS\n",
    "distances, indices = faiss_index.search(embedding.numpy(), k=5)\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

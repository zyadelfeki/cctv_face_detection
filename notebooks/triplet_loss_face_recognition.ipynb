{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üîê Face Recognition with Triplet Loss & Unknown Detection\n",
    "\n",
    "## üìã Project Overview\n",
    "\n",
    "**Objective:** Closed-set face recognition with unknown detection\n",
    "- **Known Identities:** 10 criminals\n",
    "- **Images per person:** 20 face images\n",
    "- **Unknown Detection:** Faces not in dataset classified as \"Unknown\"\n",
    "\n",
    "## üéØ Architecture\n",
    "\n",
    "- **Backbone:** ResNet50 (pretrained on ImageNet)\n",
    "- **Loss Function:** Triplet Loss with hard negative mining\n",
    "- **Embedding Size:** 128D\n",
    "- **Distance Metric:** Cosine Similarity\n",
    "- **Unknown Threshold:** Learned during validation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'‚úÖ Device: {DEVICE}')\n",
    "print(f'üì¶ PyTorch: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_section",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation\n",
    "\n",
    "Expected structure:\n",
    "```\n",
    "data/\n",
    "  person_0/\n",
    "    img_00.jpg\n",
    "    img_01.jpg\n",
    "    ...\n",
    "    img_19.jpg\n",
    "  person_1/\n",
    "    ...\n",
    "  ...\n",
    "  person_9/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletFaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Face Recognition Dataset with Triplet Mining\n",
    "    Returns: (anchor, positive, negative) triplets\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str, transform=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Build dataset index\n",
    "        self.identities = sorted([d.name for d in self.data_dir.iterdir() if d.is_dir()])\n",
    "        self.identity_to_idx = {name: idx for idx, name in enumerate(self.identities)}\n",
    "        \n",
    "        # Group images by identity\n",
    "        self.identity_images = defaultdict(list)\n",
    "        self.all_images = []\n",
    "        \n",
    "        for identity in self.identities:\n",
    "            identity_dir = self.data_dir / identity\n",
    "            images = list(identity_dir.glob('*.jpg')) + list(identity_dir.glob('*.png')) + list(identity_dir.glob('*.jpeg'))\n",
    "            \n",
    "            for img_path in images:\n",
    "                self.identity_images[identity].append(str(img_path))\n",
    "                self.all_images.append((str(img_path), identity))\n",
    "        \n",
    "        print(f'üìä Dataset Summary:')\n",
    "        print(f'   Identities: {len(self.identities)}')\n",
    "        print(f'   Total images: {len(self.all_images)}')\n",
    "        for identity in self.identities:\n",
    "            print(f'   {identity}: {len(self.identity_images[identity])} images')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get anchor\n",
    "        anchor_path, anchor_identity = self.all_images[idx]\n",
    "        anchor_img = self._load_image(anchor_path)\n",
    "        \n",
    "        # Get positive (same identity, different image)\n",
    "        positive_path = random.choice([p for p in self.identity_images[anchor_identity] if p != anchor_path])\n",
    "        positive_img = self._load_image(positive_path)\n",
    "        \n",
    "        # Get negative (different identity)\n",
    "        negative_identity = random.choice([i for i in self.identities if i != anchor_identity])\n",
    "        negative_path = random.choice(self.identity_images[negative_identity])\n",
    "        negative_img = self._load_image(negative_path)\n",
    "        \n",
    "        # Get label\n",
    "        label = self.identity_to_idx[anchor_identity]\n",
    "        \n",
    "        return anchor_img, positive_img, negative_img, label\n",
    "    \n",
    "    def _load_image(self, path: str):\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transforms",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print('‚úÖ Transforms defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "DATA_DIR = './data'  # Update this path\n",
    "\n",
    "# Create train/val datasets\n",
    "train_dataset = TripletFaceDataset(DATA_DIR, transform=train_transform)\n",
    "val_dataset = TripletFaceDataset(DATA_DIR, transform=val_transform)\n",
    "\n",
    "# DataLoaders\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'\\n‚úÖ DataLoaders ready')\n",
    "print(f'   Train batches: {len(train_loader)}')\n",
    "print(f'   Val batches: {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_section",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "Transfer Learning with ResNet50 backbone + Custom Embedding Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceEmbeddingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Face Recognition Model with Triplet Loss\n",
    "    - Pretrained ResNet50 backbone\n",
    "    - Custom embedding head (128D)\n",
    "    - L2 normalized embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_size=128, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained ResNet50\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # Remove final FC layer\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "        # Freeze early layers (fine-tune only last blocks)\n",
    "        for param in list(self.backbone.parameters())[:-20]:\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Embedding head\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, embedding_size)\n",
    "        )\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Get embeddings\n",
    "        embeddings = self.embedding(features)\n",
    "        \n",
    "        # L2 normalize\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = FaceEmbeddingModel(embedding_size=128, pretrained=True).to(DEVICE)\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'\\nüìê Model Summary:')\n",
    "print(f'   Total parameters: {total_params:,}')\n",
    "print(f'   Trainable parameters: {trainable_params:,}')\n",
    "print(f'   Embedding size: {model.embedding_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss_section",
   "metadata": {},
   "source": [
    "## 4. Triplet Loss Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "triplet_loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet Loss with online hard negative mining\n",
    "    L = max(0, ||a - p||¬≤ - ||a - n||¬≤ + margin)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Compute pairwise distances\n",
    "        pos_dist = F.pairwise_distance(anchor, positive, p=2)\n",
    "        neg_dist = F.pairwise_distance(anchor, negative, p=2)\n",
    "        \n",
    "        # Triplet loss\n",
    "        loss = F.relu(pos_dist - neg_dist + self.margin)\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# Initialize loss and optimizer\n",
    "criterion = TripletLoss(margin=0.5)\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print('‚úÖ Loss function and optimizer configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_section",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    \n",
    "    for anchor, positive, negative, _ in pbar:\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get embeddings\n",
    "        anchor_emb = model(anchor)\n",
    "        positive_emb = model(positive)\n",
    "        negative_emb = model(negative)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "val_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate and compute verification accuracy\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Store embeddings and labels\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Validating')\n",
    "    \n",
    "    for anchor, positive, negative, labels in pbar:\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        anchor_emb = model(anchor)\n",
    "        positive_emb = model(positive)\n",
    "        negative_emb = model(negative)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Store embeddings\n",
    "        all_embeddings.append(anchor_emb.cpu())\n",
    "        all_labels.extend(labels.numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    # Compute verification accuracy (positive pairs should be closer than negative)\n",
    "    all_embeddings = torch.cat(all_embeddings)\n",
    "    \n",
    "    # Simple verification: for each anchor, check if positive is closer than negative\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(0, len(all_embeddings), len(loader.dataset) // len(loader)):\n",
    "        if i + 2 >= len(all_embeddings):\n",
    "            break\n",
    "        anchor = all_embeddings[i]\n",
    "        positive = all_embeddings[i + 1] if i + 1 < len(all_embeddings) else all_embeddings[i]\n",
    "        negative = all_embeddings[i + 2] if i + 2 < len(all_embeddings) else all_embeddings[i]\n",
    "        \n",
    "        pos_dist = F.pairwise_distance(anchor.unsqueeze(0), positive.unsqueeze(0))\n",
    "        neg_dist = F.pairwise_distance(anchor.unsqueeze(0), negative.unsqueeze(0))\n",
    "        \n",
    "        if pos_dist < neg_dist:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    return total_loss / num_batches, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 50\n",
    "SAVE_DIR = Path('./checkpoints')\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "best_val_acc = 0\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print('üöÄ Starting training...\\n')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'\\nEpoch {epoch + 1}/{NUM_EPOCHS}')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log\n",
    "    print(f'\\nTrain Loss: {train_loss:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc*100:.2f}%')\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_accuracy': best_val_acc,\n",
    "            'embedding_size': model.embedding_size,\n",
    "            'identities': train_dataset.identities\n",
    "        }, SAVE_DIR / 'model.pth')\n",
    "        print(f'üíæ Saved best model with accuracy: {best_val_acc*100:.2f}%')\n",
    "\n",
    "print(f'\\n‚úÖ Training complete! Best validation accuracy: {best_val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_section",
   "metadata": {},
   "source": [
    "## 6. Inference & Unknown Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceRecognitionSystem:\n",
    "    \"\"\"\n",
    "    Face Recognition System with Unknown Detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, identities, threshold=0.6):\n",
    "        self.model = model\n",
    "        self.identities = identities\n",
    "        self.threshold = threshold\n",
    "        self.embeddings_db = {}\n",
    "        self.model.eval()\n",
    "    \n",
    "    def register_identity(self, identity_name: str, image_paths: List[str], transform):\n",
    "        \"\"\"Register an identity by computing average embedding\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for img_path in image_paths:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = transform(img).unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                emb = self.model(img)\n",
    "                embeddings.append(emb.cpu())\n",
    "        \n",
    "        # Average embedding\n",
    "        avg_embedding = torch.cat(embeddings).mean(dim=0)\n",
    "        self.embeddings_db[identity_name] = avg_embedding\n",
    "    \n",
    "    def predict(self, image_path: str, transform) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Predict identity for a face image\n",
    "        Returns: (identity_name, confidence) or ('Unknown', distance)\n",
    "        \"\"\"\n",
    "        # Load and process image\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img = transform(img).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        # Get embedding\n",
    "        with torch.no_grad():\n",
    "            query_emb = self.model(img).cpu()\n",
    "        \n",
    "        # Compare with database\n",
    "        best_match = None\n",
    "        best_similarity = -1\n",
    "        \n",
    "        for identity, db_emb in self.embeddings_db.items():\n",
    "            # Cosine similarity\n",
    "            similarity = F.cosine_similarity(query_emb, db_emb.unsqueeze(0)).item()\n",
    "            \n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_match = identity\n",
    "        \n",
    "        # Check threshold\n",
    "        if best_similarity >= self.threshold:\n",
    "            return best_match, best_similarity\n",
    "        else:\n",
    "            return 'Unknown', best_similarity\n",
    "\n",
    "\n",
    "print('‚úÖ Inference system ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "register_identities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(SAVE_DIR / 'model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Create recognition system\n",
    "recognition_system = FaceRecognitionSystem(\n",
    "    model=model,\n",
    "    identities=checkpoint['identities'],\n",
    "    threshold=0.6  # Adjust based on validation\n",
    ")\n",
    "\n",
    "# Register all identities\n",
    "print('üìù Registering identities...')\n",
    "for identity in train_dataset.identities:\n",
    "    image_paths = train_dataset.identity_images[identity]\n",
    "    recognition_system.register_identity(identity, image_paths, val_transform)\n",
    "    print(f'   ‚úÖ {identity}')\n",
    "\n",
    "print(f'\\n‚úÖ System ready with {len(recognition_system.embeddings_db)} identities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "test_image_path = './data/person_0/img_00.jpg'  # Update with your test image\n",
    "\n",
    "identity, confidence = recognition_system.predict(test_image_path, val_transform)\n",
    "\n",
    "print(f'\\nüéØ Prediction:')\n",
    "print(f'   Identity: {identity}')\n",
    "print(f'   Confidence: {confidence:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot([a*100 for a in history['val_acc']], label='Val Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAVE_DIR / 'training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## üìä Summary\n",
    "\n",
    "### Model Architecture\n",
    "- **Backbone:** ResNet50 (pretrained)\n",
    "- **Embedding Size:** 128D\n",
    "- **Loss:** Triplet Loss (margin=0.5)\n",
    "- **Metric:** Cosine Similarity\n",
    "\n",
    "### Training Details\n",
    "- **Known Identities:** 10 persons\n",
    "- **Images per person:** 20\n",
    "- **Epochs:** 50\n",
    "- **Batch Size:** 16\n",
    "\n",
    "### Inference\n",
    "- **Threshold:** 0.6 (adjustable)\n",
    "- **Unknown Detection:** Yes\n",
    "- **Output:** Identity + Confidence score\n",
    "\n",
    "### Files Generated\n",
    "- `checkpoints/model.pth` - Best model weights\n",
    "- `checkpoints/training_history.png` - Training curves\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for deployment!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}